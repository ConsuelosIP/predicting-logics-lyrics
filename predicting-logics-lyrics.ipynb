{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Logic's Lyrics with Machine Learning\n",
    "### Hans Kamin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logic** has been a remarkable influence on my life since middle school, when I heard his song [“All I Do”](https://www.youtube.com/watch?v=eIGh4Nc1fAM) for the first time. All avid music fans have those select few artists in their library whom they’ll never stop listening to; the connections these musicians make with our emotions & memories become so firmly rooted that we simply can’t help but enjoy each piece of work they put out. Such is absolutely the case in my relationship with Logic’s music, so I instantly knew whose lyrics I’d choose when I was presented with this project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than any other coding assignment I’ve been given, this project was by far the most intriguing and exhilarating to me, so I decided to write about how one might go about implementing the algorithm. I’ll walk through and provide the Python code I wrote and then discuss some of the strengths and weaknesses of this implementation, as well as how it can be improved in the future. And, of course, a *humongous* shoutout to Professor Dennis Sun at Cal Poly SLO for providing excellent solutions and help, and for assigning such an awesome lab through which to explore data science!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, though, it would be wise to _**[visit this webpage](http://setosa.io/ev/markov-chains/)**_ for a detailed, visual explanation of Markov Chains and how they work—this is crucial to understanding how to approach this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Algorithm\n",
    "The crux of this implementation involves using a Bigram Markov Chain to represent the English language. More specifically, our chain will be a dictionary object in which each key is a unique tuple consisting of a word and the word that follows it. Using bigrams rather than single words allows us to increase the accuracy and readability of our generated lines because it defines our model such that the next word in our sentence is predicted based on the previous two words rather than just the immediately preceding one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true first step, though, is to gather all of the lyrics we’ll be analyzing. To do this, I scraped links to lyrics for each of Logic’s songs, then went through each link to gather all of the lyrics from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've been on the low\n",
      "I been taking my time\n",
      "I feel like I'm out of my mind\n",
      "It feel like my life ain't mine\n",
      "Who can relate?\n",
      "I've been on the low\n",
      "I been taking my time\n",
      "I feel like I'm out of my mind\n",
      "It feel like my life ain't mine\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "links = []\n",
    "for pagenum in range(1,3):\n",
    "    url = \"http://www.metrolyrics.com/logic-alpage-%d.html\" % pagenum\n",
    "    soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n",
    "    # First table on this page contains links to songs by Logic.\n",
    "    table = soup.find(\"table\")\n",
    "    for song in table.find_all('a'):\n",
    "        links.append(song.get(\"href\"))\n",
    "\n",
    "# Enter each link and scrape all of the lyrics.\n",
    "# Each element in our lyrics list will pertain to one song.\n",
    "# Parsing through each link takes a while, expect long runtime.\n",
    "lyrics = []\n",
    "for link in links:\n",
    "    time.sleep(0.1)\n",
    "    bs = BeautifulSoup(requests.get(link).text, \"html.parser\")\n",
    "    paragraphs = bs.find_all('p')\n",
    "    song_text = \"\"\n",
    "    for p in paragraphs:\n",
    "        if p.get(\"class\") != None and \"verse\" in p.get(\"class\"):\n",
    "            song_text = song_text + p.text\n",
    "    lyrics.append(song_text)\n",
    "\n",
    "# Print out the lyrics to the first song.\n",
    "print(lyrics[0][:227])\n",
    "\n",
    "# `pickle` is a Python package that serializes Python objects to disk so that you can load them in later.\n",
    "import pickle\n",
    "pickle.dump(lyrics, open(\"lyrics.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result is a list in which each element is a string containing all of the lyrics to one song."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally dig into building our Markov Chain. We write a function that iterates through each word in all of Logic’s lyrics in order to generate the model we discussed before by examining each sequence of two words and creating a list of all of the words that follow each sequence. For more efficient/practical iteration, we use `\"<START>\"`, `\"<END>\"`, and `\"<N>\"` tags to represent a song’s beginning, its end, and its newline characters, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_markov_chain(lyrics):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      - lyrics: a list of strings, where each string represents\n",
    "                the lyrics of one song by an artist.\n",
    "    Returns:\n",
    "      A dict that maps a tuple of 2 words (\"bigram\") to a list of\n",
    "      words that follow that bigram, representing the Markov\n",
    "      chain trained on the lyrics.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the beginning of our chain.\n",
    "    chain = {\n",
    "        (None, \"<START>\"): []\n",
    "    }\n",
    "\n",
    "    for lyric in lyrics:\n",
    "        # Replace newline characters with our tag.\n",
    "        lyric_newlines = lyric.replace('\\n', ' <N> ')\n",
    "        # Create a tuple representing the most recent (current) bigram.\n",
    "        last_2 = (None, \"<START>\")\n",
    "        for word in lyric_newlines.split():\n",
    "            # Add the word as one that follows the current bigram.\n",
    "            chain[last_2].append(word)\n",
    "            # Shift the current bigram to account for the newly added word.\n",
    "            last_2 = (last_2[1], word)\n",
    "            if last_2 not in chain:\n",
    "                chain[last_2] = []\n",
    "        chain[last_2].append(\"<END>\")\n",
    "\n",
    "    return chain\n",
    "\n",
    "# Load the pickled lyrics object that we created earlier.\n",
    "import pickle\n",
    "lyrics = pickle.load(open(\"lyrics.pkl\", \"rb\"))\n",
    "\n",
    "# Train a Markov Chain over all of Logic's lyrics.\n",
    "chain = train_markov_chain(lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we’ve built and returned the dictionary representing our Markov Chain, we can move onto the final portion of the algorithm: generating predicted lyrics. Beginning from the `(None, \"<START>\")` key (the first key in our chain), we randomly sample one of the words in the list connected to that key, then shift the key we’re currently examining to account for the word we just sampled. We continue this process all the way through until the `\"<END>\"` tag is finally encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_new_lyrics(chain):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      - chain: a dict representing the Markov chain,\n",
    "               such as one generated by generate_new_lyrics()\n",
    "    Returns:\n",
    "      A string representing the randomly generated song.\n",
    "    \"\"\"\n",
    "\n",
    "    # a list for storing the generated words\n",
    "    words = []\n",
    "    # generate the first word\n",
    "    word = random.choice(chain[(None, \"<START>\")])\n",
    "    words.append(word)\n",
    "\n",
    "    # Begin with the first bigram in our chain.\n",
    "    last_2 = (None, \"<START>\")\n",
    "    while words[-1] != \"<END>\":\n",
    "        # Generate the next word.\n",
    "        word = random.choice(chain[last_2])\n",
    "        words.append(word)\n",
    "        # Shift the current bigram to account for the newly added word.\n",
    "        last_2 = (last_2[1], words[-1])\n",
    "\n",
    "    # Join the words together into a string with line breaks.\n",
    "    lyrics = \" \".join(words[:-1])\n",
    "    return \"\\n\".join(lyrics.split(\"<N>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can now `print(generate_new_lyrics(chain))` to display our predicted lyrics in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hook] Living life behind the scenes \n",
      " He one of us \n",
      " Had my share of stealin' \n",
      " But it doesn't matter, homie I got robbed \n",
      " Life is moving fast it need to pretend Imma never do it for the show and come back Monday with a gun to bust \n",
      " Police looking for talent, I hope that you can't be playin' that, that's the album \n",
      " That's what's so crazy because I've always-like I'm on this killing spree \n",
      " All the grass is green \n",
      " And now I hate shots \n",
      " That's that road I been lookin' for something \n",
      " Everybody want me to be bumping this shit \n",
      " Yeah I've been, yeah I've been vibing out here tryna gas it up for these insta folk \n",
      " And I really do anything, you can or can't be off right \n",
      " But now I needBlack people: to just fight, fight for ya life \n",
      " Yeah, your girl on a Monday \n",
      " Drop hits, get money \n",
      " All the haters and what the drama bring, \n",
      " It ain't about it \n",
      " But I was like 15 for the music, I'm gone \n",
      " Yes, I know \n",
      " Probably wanna fight like this \n",
      " Stab a motherfucker bleed, yeah theyFuckin with me now, fuckin with me now, fuckin with me and my homies, we tryna make it with no motherfucking miles on em' \n",
      " But I'm so lucky to be alive \n",
      " I spit this verse is heaven sent, never irrelevant \n",
      " No one tells you you're that \n",
      " 'Cause y'all just watched the throne, I'm tryna remember right now, \n",
      " Please don't stop the spit from profit \n",
      " But the devil finna come and get in 'em I'm out for presidents to represent me \n",
      " She said that he did not know \n",
      " It feel like the devil said that just means like a goddamn king \n",
      " Pawns tend to carry on with no basic bitch \n",
      " And you might not not not not not be captured \n",
      " The Return of the first black man \n",
      " Hold up, whats the hold up \n",
      " Bitch I run that \n",
      " Get the fuck you know it's hard, but we do it) \n",
      " And not with semi automatic bullets in the dead of it goes away. It's like I didn't care what whites thought \n",
      " I'd be better than you, you, you \n",
      " But tell me how you're feelin', higher than the street going ape shit \n",
      " Aw yeah I'm back in this bitch \n",
      " And I know what I'm giving, I'ma never give in, thats a given. \n",
      " I'm a slave (alright) \n",
      " Hey-oh, hey, that hey-oh, hey-oh, hey \n",
      " Hey-oh, hey, that hey-oh, hey-oh, hey \n",
      " Hey-oh, hey-oh, hey (alright, lets go) \n",
      " Hey-oh, hey-oh (let's go) \n",
      " Hey-oh, hey-oh (let's go) \n",
      " Hey-oh, hey-oh, hey-ey-oh \n",
      " Hey-oh, hey, that hey-oh, hey-oh, hey (alright, lets go) \n",
      " Hey-oh (alright) \n",
      " Hey-oh, hey-oh (let's go)East side, west side, we ride, we die all for this \n",
      " So live your life to the streets even though they undershipped me \n",
      " I'm likeHold on let me tell you \n",
      " Maybe if I do is legendary \n",
      " And you don't like Tarantino \n",
      " Ugh, I don't wanna die) \n",
      " I think they know what's up \n",
      " But never acknowledge it \n",
      " I just grew and decided to put you in mind, you're so fine \n",
      " Everything will be okay \n",
      " I know I do what I thought I should be black \n",
      " If you try \n",
      " This shit right here, no it can't be \n",
      " Tell def jam if they notice me \n",
      " I know you are a boss. You're a boss, and you send me back at it way way back \n",
      " Now, Aye girl aye girl yeah I'm tryna do just what the fuck do you listen to lyrics? \n",
      " He's making a difference make a move and feel like my dream is a whole other concept album (this is fucking crazy. Like, I'm sorry, your highness. \n",
      " I'll tell you whyAll this other shit I gotta blow \n",
      " That made him smile though his eyes said, \"Pray for me\" \n",
      " I'll have to give right nowI've been on \n",
      " Fuckin' with your mind, tryna turn shit on 'em by surprise like I'm Capone \n",
      " Reep what I've sewn, in other words that is so intense \n",
      " Don't let me say yes I spit at it way back when, now let me tell you 'bout a thing \n",
      " Light skin mothafucka certified as a house \n",
      " Chillin' with my homies and we kill each other \n",
      " But lately I don't know why I got an appointment. What's your name man? \n",
      " Fan: Logic? \n",
      " I just got some great things that glistenI know my mind \n",
      " It feel like \n",
      " putting together a trailer for a sign right now because I had to do \n",
      " This entire place was made for you in your blood, you was born with the contributing to the top off and I go \n",
      " And I'm gone, so long, I'm gone, accept Him\n"
     ]
    }
   ],
   "source": [
    "print(generate_new_lyrics(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s imperative to note, however, that because I use simple random sampling to create new lyrics, I’m also randomizing how much output I actually receive. There were a select few instances in which I received less than one line or even just one word of output, but most of the time the algorithm printed out a giant amount of lyrics. Nonetheless, after searching through the outputs I received from many runs of the algorithm, I got a handful of pretty good lyrics overall, ranging from raw punchlines to downright hilarious quips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Our Results\n",
    "Observing many, many outputs from our implementation and those from a unigram implementation allows us to reach some important conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Our model’s predictions are accurate, but often recycled.** It’s important to note that many of our predicted lines turned out to be nearly identical to lines Logic has actually written, i.e. half of a line from one verse/song combined with half of a line from another verse/song. This is to be expected, as using bigrams yields less variability in predicted words due to basing predictions off the previous two words instead of the previous one, resulting in sequences of three or more words coming from the same Logic lyric. In other words, *using bigrams instead of single words increases readability and similarity to Logic’s style, but decreases creativity.*\n",
    "2. **Our model is slower and generates less output.** The unigram model runs faster because the dictionary object representing its Markov Chain has far fewer keys. Our model has so many more keys because it has to process tuples of two words. Furthermore, as I mentioned before, there were times when I received very little to no output, and generally I received less than I did from the unigram implementation. This can be attributed to the smaller number of possibilities for the next word when we’re basing it off the previous two words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So where do we go from here? We’ve highlighted the strengths and weaknesses of our implementation; how do we actually mitigate those weaknesses and make our model even better? The key to discovering a superior design is to first discern the central Markov Assumption limiting the model that we built."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding A Better Way\n",
    "Modeling a situation with a Markov Chain requires assuming that the situation satisfies one key statement: a prediction for the next state of the situation only depends on the current state, not the rest of the situation’s history. For example, using Markov Chains to predict tomorrow’s weather involves the assumption that weather from the past two weeks or more has no effect on tomorrow’s conditions—something I think we can all agree sounds pretty far-fetched. Thus, even though using bigrams helped us decrease the magnitude of this assumption in our model, its impact was still prevalent and weakened our results. We need to find an alternative to our model that can at the very least make fewer assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent neural network is one example of a replacement we can use. While I won’t go into much detail here about RNNs, mostly because I’m still only cracking the surface with them myself, I can provide some very brief notes on them. Two of the key characteristics of RNNs are that they don’t assume that all inputs are independent of each other and that they’re capable of keeping a history of what they’ve processed, both of which are necessary to improving our model. For more information on RNNs, how they work, and how to implement them, check out the [Wikipedia page](https://en.wikipedia.org/wiki/Recurrent_neural_network) as well as [this tutorial](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/); I’ll be learning from both to eventually create better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’ve made it this far, thanks for reading about and (hopefully) understanding my newfound interest in machine learning! Data science as a whole already has so many fascinating and creative applications. I look forward to exploring the many nuances and intricacies in further detail as I work on more projects and continue to improve. After all, as Logic once wrote (and Paul Brandt before him), how can the sky be the limit when there are footprints on the moon?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
